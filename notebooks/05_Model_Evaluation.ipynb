{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 05. Model Evaluation\n",
        "\n",
        "Notebook này đánh giá hiệu suất mô hình:\n",
        "- Metrics (Accuracy, Precision, Recall, F1, ROC-AUC)\n",
        "- Confusion Matrix\n",
        "- ROC Curve\n",
        "- Precision-Recall Curve\n",
        "- So sánh models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Import project modules\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Calculate project root by finding directory with config folder\n",
        "current_path = Path().resolve()\n",
        "project_root = current_path\n",
        "\n",
        "# Find project root by looking for config/ directory\n",
        "max_levels = 5\n",
        "for _ in range(max_levels):\n",
        "    if (project_root / 'config').exists() and (project_root / 'src').exists():\n",
        "        break\n",
        "    if project_root.parent == project_root:\n",
        "        break\n",
        "    project_root = project_root.parent\n",
        "else:\n",
        "    if 'notebooks' in str(current_path):\n",
        "        project_root = current_path.parent\n",
        "\n",
        "# Add src to Python path\n",
        "src_path = project_root / 'src'\n",
        "if src_path.exists():\n",
        "    sys.path.insert(0, str(src_path))\n",
        "\n",
        "from models.decision_tree import load_model as load_dt_model\n",
        "from models.random_forest import load_model as load_rf_model\n",
        "from models.train import predict, predict_proba\n",
        "from evaluation.metrics import evaluate_model, print_classification_report\n",
        "from evaluation.visualization import plot_confusion_matrix, plot_roc_curve, plot_precision_recall_curve"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load Models and Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Models and data loaded successfully!\n"
          ]
        }
      ],
      "source": [
        "# Load models\n",
        "dt_model = load_dt_model('decision_tree.pkl')\n",
        "rf_model = load_rf_model('random_forest.pkl')\n",
        "\n",
        "# Load test data\n",
        "processed_dir = project_root / \"data\" / \"processed\"\n",
        "X_test = pd.read_csv(processed_dir / \"X_test.csv\")\n",
        "y_test = pd.read_csv(processed_dir / \"y_test.csv\").squeeze()\n",
        "\n",
        "print(\"Models and data loaded successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Evaluate Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Decision Tree Evaluation ===\n",
            "\n"
          ]
        },
        {
          "ename": "ImportError",
          "evalue": "attempted relative import beyond top-level package",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Evaluate Decision Tree\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=== Decision Tree Evaluation ===\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m dt_metrics = \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdt_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mMetrics:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m metric, value \u001b[38;5;129;01min\u001b[39;00m dt_metrics.items():\n",
            "\u001b[36mFile \u001b[39m\u001b[32mD:\\code\\machine\\bai-cuoi-ky\\src\\evaluation\\metrics.py:110\u001b[39m, in \u001b[36mevaluate_model\u001b[39m\u001b[34m(model, X, y, preprocessor)\u001b[39m\n\u001b[32m     90\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mevaluate_model\u001b[39m(model, X, y, preprocessor=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m     91\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     92\u001b[39m \u001b[33;03m    Evaluate a trained model on a dataset.\u001b[39;00m\n\u001b[32m     93\u001b[39m \u001b[33;03m    \u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    108\u001b[39m \u001b[33;03m        Dictionary of metrics.\u001b[39;00m\n\u001b[32m    109\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m110\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtrain\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m predict, predict_proba\n\u001b[32m    112\u001b[39m     y_pred = predict(model, X, preprocessor)\n\u001b[32m    114\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
            "\u001b[31mImportError\u001b[39m: attempted relative import beyond top-level package"
          ]
        }
      ],
      "source": [
        "# Evaluate Decision Tree\n",
        "print(\"=== Decision Tree Evaluation ===\\n\")\n",
        "\n",
        "dt_metrics = evaluate_model(dt_model, X_test, y_test)\n",
        "print(\"Metrics:\")\n",
        "for metric, value in dt_metrics.items():\n",
        "    print(f\"{metric}: {value:.4f}\")\n",
        "\n",
        "# Predictions\n",
        "y_pred_dt = predict(dt_model, X_test)\n",
        "y_proba_dt = predict_proba(dt_model, X_test)[:, 1] if hasattr(dt_model, 'predict_proba') else None\n",
        "\n",
        "# Classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print_classification_report(y_test, y_pred_dt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Evaluate Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate Random Forest\n",
        "print(\"=== Random Forest Evaluation ===\\n\")\n",
        "\n",
        "rf_metrics = evaluate_model(rf_model, X_test, y_test)\n",
        "print(\"Metrics:\")\n",
        "for metric, value in rf_metrics.items():\n",
        "    print(f\"{metric}: {value:.4f}\")\n",
        "\n",
        "# Predictions\n",
        "y_pred_rf = predict(rf_model, X_test)\n",
        "y_proba_rf = predict_proba(rf_model, X_test)[:, 1] if hasattr(rf_model, 'predict_proba') else None\n",
        "\n",
        "# Classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print_classification_report(y_test, y_pred_rf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Visualizations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot confusion matrices\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Decision Tree CM\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm_dt = confusion_matrix(y_test, y_pred_dt)\n",
        "sns.heatmap(cm_dt, annot=True, fmt='d', ax=axes[0], cmap='Blues')\n",
        "axes[0].set_title('Decision Tree - Confusion Matrix')\n",
        "\n",
        "# Random Forest CM\n",
        "cm_rf = confusion_matrix(y_test, y_pred_rf)\n",
        "sns.heatmap(cm_rf, annot=True, fmt='d', ax=axes[1], cmap='Blues')\n",
        "axes[1].set_title('Random Forest - Confusion Matrix')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ROC Curves\n",
        "if y_proba_dt is not None and y_proba_rf is not None:\n",
        "    from sklearn.metrics import roc_curve, auc\n",
        "    \n",
        "    fpr_dt, tpr_dt, _ = roc_curve(y_test, y_proba_dt)\n",
        "    fpr_rf, tpr_rf, _ = roc_curve(y_test, y_proba_rf)\n",
        "    \n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(fpr_dt, tpr_dt, label=f'Decision Tree (AUC = {auc(fpr_dt, tpr_dt):.3f})')\n",
        "    plt.plot(fpr_rf, tpr_rf, label=f'Random Forest (AUC = {auc(fpr_rf, tpr_rf):.3f})')\n",
        "    plt.plot([0, 1], [0, 1], 'k--', label='Random')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('ROC Curves Comparison')\n",
        "    plt.legend()\n",
        "    plt.grid(alpha=0.3)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Model Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare models\n",
        "comparison = pd.DataFrame({\n",
        "    'Decision Tree': dt_metrics,\n",
        "    'Random Forest': rf_metrics\n",
        "})\n",
        "print(\"Model Comparison:\")\n",
        "print(comparison)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
