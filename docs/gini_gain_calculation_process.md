# M√¥ H√¨nh H√≥a Quy Tr√¨nh T√≠nh Gini Gain Cho T·∫•t C·∫£ Features

## 1. T·ªïng Quan Quy Tr√¨nh

Khi Decision Tree c·∫ßn ch·ªçn best split t·∫°i m·ªôt node, n√≥ ph·∫£i:

1. **T√≠nh Gini Gain cho T·∫§T C·∫¢ features** trong dataset (34 features sau feature engineering)
2. **V·ªõi m·ªói feature:** Th·ª≠ t·∫•t c·∫£ c√°c c√°ch split kh·∫£ thi
3. **So s√°nh** t·∫•t c·∫£ c√°c Gini Gain
4. **Ch·ªçn** feature + threshold c√≥ **Gini Gain cao nh·∫•t**

---

## 2. Flowchart Quy Tr√¨nh

```
START: Node c·∫ßn split
  ‚îÇ
  ‚îú‚îÄ> T√≠nh Gini(parent) c·ªßa node hi·ªán t·∫°i
  ‚îÇ
  ‚îú‚îÄ> FOR m·ªói feature trong [34 features]:
  ‚îÇ     ‚îÇ
  ‚îÇ     ‚îú‚îÄ> IF feature l√† NUMERICAL:
  ‚îÇ     ‚îÇ     ‚îÇ
  ‚îÇ     ‚îÇ     ‚îú‚îÄ> S·∫Øp x·∫øp gi√° tr·ªã feature
  ‚îÇ     ‚îÇ     ‚îÇ
  ‚îÇ     ‚îÇ     ‚îú‚îÄ> T·∫°o c√°c threshold candidates:
  ‚îÇ     ‚îÇ     ‚îÇ     - Trung b√¨nh gi·ªØa c√°c gi√° tr·ªã li√™n ti·∫øp
  ‚îÇ     ‚îÇ     ‚îÇ     - V√≠ d·ª•: [1, 3, 5] ‚Üí thresholds: [2, 4]
  ‚îÇ     ‚îÇ     ‚îÇ
  ‚îÇ     ‚îÇ     ‚îú‚îÄ> FOR m·ªói threshold:
  ‚îÇ     ‚îÇ     ‚îÇ     ‚îÇ
  ‚îÇ     ‚îÇ     ‚îÇ     ‚îú‚îÄ> Split: feature <= threshold vs feature > threshold
  ‚îÇ     ‚îÇ     ‚îÇ     ‚îÇ
  ‚îÇ     ‚îÇ     ‚îÇ     ‚îú‚îÄ> T√≠nh Weighted Gini(children)
  ‚îÇ     ‚îÇ     ‚îÇ     ‚îÇ
  ‚îÇ     ‚îÇ     ‚îÇ     ‚îú‚îÄ> T√≠nh Gini Gain = Gini(parent) - Weighted Gini(children)
  ‚îÇ     ‚îÇ     ‚îÇ     ‚îÇ
  ‚îÇ     ‚îÇ     ‚îÇ     ‚îî‚îÄ> L∆∞u (feature, threshold, gini_gain)
  ‚îÇ     ‚îÇ     ‚îÇ
  ‚îÇ     ‚îÇ     ‚îî‚îÄ> END FOR threshold
  ‚îÇ     ‚îÇ
  ‚îÇ     ‚îî‚îÄ> IF feature l√† CATEGORICAL/BINARY:
  ‚îÇ           ‚îÇ
  ‚îÇ           ‚îú‚îÄ> FOR m·ªói gi√° tr·ªã unique c·ªßa feature:
  ‚îÇ           ‚îÇ     ‚îÇ
  ‚îÇ           ‚îÇ     ‚îú‚îÄ> Split: feature == value vs feature != value
  ‚îÇ           ‚îÇ     ‚îÇ     (ho·∫∑c split th√†nh nhi·ªÅu nh√≥m n·∫øu nhi·ªÅu gi√° tr·ªã)
  ‚îÇ           ‚îÇ     ‚îÇ
  ‚îÇ           ‚îÇ     ‚îú‚îÄ> T√≠nh Weighted Gini(children)
  ‚îÇ           ‚îÇ     ‚îÇ
  ‚îÇ           ‚îÇ     ‚îú‚îÄ> T√≠nh Gini Gain
  ‚îÇ           ‚îÇ     ‚îÇ
  ‚îÇ           ‚îÇ     ‚îî‚îÄ> L∆∞u (feature, value, gini_gain)
  ‚îÇ           ‚îÇ
  ‚îÇ           ‚îî‚îÄ> END FOR value
  ‚îÇ
  ‚îî‚îÄ> END FOR feature
      ‚îÇ
      ‚îú‚îÄ> T√¨m (feature, threshold/value) c√≥ Gini Gain CAO NH·∫§T
      ‚îÇ
      ‚îú‚îÄ> Th·ª±c hi·ªán split v·ªõi feature v√† threshold/value ƒë√≥
      ‚îÇ
      ‚îî‚îÄ> RETURN: 2 (ho·∫∑c nhi·ªÅu) child nodes m·ªõi

END
```

---

## 3. C√¥ng Th·ª©c T√≠nh To√°n

### 3.1. Gini Impurity c·ªßa m·ªôt Node

```python
def gini_impurity(node):
    """
    T√≠nh Gini Impurity c·ªßa m·ªôt node

    Args:
        node: List c√°c labels (0 ho·∫∑c 1)

    Returns:
        Gini Impurity (0 ‚â§ Gini ‚â§ 0.5 cho binary classification)
    """
    if len(node) == 0:
        return 0

    # ƒê·∫øm s·ªë l∆∞·ª£ng m·ªói class
    count_0 = sum(1 for label in node if label == 0)
    count_1 = sum(1 for label in node if label == 1)
    total = len(node)

    # T√≠nh t·ª∑ l·ªá
    p_0 = count_0 / total
    p_1 = count_1 / total

    # C√¥ng th·ª©c Gini
    gini = 1 - (p_0**2 + p_1**2)

    return gini
```

### 3.2. Weighted Gini c·ªßa Children Nodes

```python
def weighted_gini(children):
    """
    T√≠nh Weighted Gini sau khi split th√†nh nhi·ªÅu nh√≥m

    Args:
        children: List c√°c nh√≥m con, m·ªói nh√≥m l√† list labels

    Returns:
        Weighted Gini Impurity
    """
    total_samples = sum(len(child) for child in children)

    if total_samples == 0:
        return 0

    weighted_sum = 0
    for child in children:
        weight = len(child) / total_samples
        gini_child = gini_impurity(child)
        weighted_sum += weight * gini_child

    return weighted_sum
```

### 3.3. Gini Gain

```python
def gini_gain(parent, children):
    """
    T√≠nh Gini Gain = s·ª± gi·∫£m Gini sau khi split

    Args:
        parent: List labels c·ªßa node cha
        children: List c√°c nh√≥m con sau split

    Returns:
        Gini Gain (c√†ng cao c√†ng t·ªët)
    """
    gini_parent = gini_impurity(parent)
    weighted_gini_children = weighted_gini(children)

    gain = gini_parent - weighted_gini_children

    return gain
```

---

## 4. V√≠ D·ª• Minh H·ªça C·ª• Th·ªÉ

### Dataset M·∫´u (5 m·∫´u ƒë·ªÉ minh h·ªça):

| Row | PageValues | Weekend | BounceRates | Revenue |
| --- | ---------- | ------- | ----------- | ------- |
| 1   | 0.0        | 0       | 0.2         | **1**   |
| 2   | 0.0        | 1       | 0.0         | 0       |
| 3   | 0.0        | 1       | 0.2         | 0       |
| 4   | 0.0        | 1       | 0.05        | 0       |
| 5   | 10.0       | 1       | 0.02        | 0       |

**Node cha:** `[1, 0, 0, 0, 0]` ‚Üí 1 Revenue=1, 4 Revenue=0

### B∆∞·ªõc 1: T√≠nh Gini(parent)

```python
# Node cha: [1, 0, 0, 0, 0]
count_0 = 4
count_1 = 1
total = 5

p_0 = 4/5 = 0.8
p_1 = 1/5 = 0.2

Gini(parent) = 1 - (0.8¬≤ + 0.2¬≤)
             = 1 - (0.64 + 0.04)
             = 1 - 0.68
             = 0.32
```

---

### B∆∞·ªõc 2: T√≠nh Gini Gain cho t·ª´ng Feature

#### Feature 1: PageValues (Numerical)

**Gi√° tr·ªã:** `[0.0, 0.0, 0.0, 0.0, 10.0]`

**Threshold candidates:**

- Gi·ªØa 0.0 v√† 10.0 ‚Üí threshold = `5.0`

**V·ªõi threshold = 5.0:**

Split:

- **Left (PageValues ‚â§ 5.0):** Rows 1,2,3,4 ‚Üí Revenue: `[1, 0, 0, 0]`
- **Right (PageValues > 5.0):** Row 5 ‚Üí Revenue: `[0]`

T√≠nh to√°n:

```python
# Left node: [1, 0, 0, 0]
Gini(left) = 1 - (0.75¬≤ + 0.25¬≤) = 1 - (0.5625 + 0.0625) = 0.375

# Right node: [0]
Gini(right) = 1 - (1¬≤ + 0¬≤) = 0

# Weighted Gini
Weighted Gini = (4/5) √ó 0.375 + (1/5) √ó 0
               = 0.8 √ó 0.375 + 0
               = 0.3

# Gini Gain
Gini Gain(PageValues, threshold=5.0) = 0.32 - 0.3 = 0.02
```

**K·∫øt qu·∫£:** Gini Gain = **0.02** (th·∫•p)

---

#### Feature 2: Weekend (Binary/Categorical)

**Gi√° tr·ªã:** `[0, 1, 1, 1, 1]`

**Split theo Weekend = 0 vs Weekend = 1:**

- **Weekend = 0:** Row 1 ‚Üí Revenue: `[1]`
- **Weekend = 1:** Rows 2,3,4,5 ‚Üí Revenue: `[0, 0, 0, 0]`

T√≠nh to√°n:

```python
# Weekend = 0 node: [1]
Gini(weekend_0) = 1 - (0¬≤ + 1¬≤) = 0  # Pure node!

# Weekend = 1 node: [0, 0, 0, 0]
Gini(weekend_1) = 1 - (1¬≤ + 0¬≤) = 0  # Pure node!

# Weighted Gini
Weighted Gini = (1/5) √ó 0 + (4/5) √ó 0
              = 0

# Gini Gain
Gini Gain(Weekend) = 0.32 - 0 = 0.32
```

**K·∫øt qu·∫£:** Gini Gain = **0.32** (r·∫•t cao!)

---

#### Feature 3: BounceRates (Numerical)

**Gi√° tr·ªã:** `[0.2, 0.0, 0.2, 0.05, 0.02]`

**S·∫Øp x·∫øp:** `[0.0, 0.02, 0.05, 0.2, 0.2]`

**Threshold candidates:**

- Gi·ªØa 0.0 v√† 0.02 ‚Üí threshold = `0.01`
- Gi·ªØa 0.02 v√† 0.05 ‚Üí threshold = `0.035`
- Gi·ªØa 0.05 v√† 0.2 ‚Üí threshold = `0.125`
- Gi·ªØa 0.2 v√† 0.2 ‚Üí b·ªè qua (gi√° tr·ªã tr√πng)

**Th·ª≠ threshold = 0.01:**

Split:

- **Left (BounceRates ‚â§ 0.01):** Rows 2,5 ‚Üí Revenue: `[0, 0]`
- **Right (BounceRates > 0.01):** Rows 1,3,4 ‚Üí Revenue: `[1, 0, 0]`

```python
# Left node: [0, 0]
Gini(left) = 1 - (1¬≤ + 0¬≤) = 0

# Right node: [1, 0, 0]
Gini(right) = 1 - (0.67¬≤ + 0.33¬≤) = 1 - (0.45 + 0.11) = 0.44

# Weighted Gini
Weighted Gini = (2/5) √ó 0 + (3/5) √ó 0.44
               = 0.264

# Gini Gain
Gini Gain(BounceRates, threshold=0.01) = 0.32 - 0.264 = 0.056
```

**K·∫øt qu·∫£:** Gini Gain = **0.056** (trung b√¨nh)

---

### B∆∞·ªõc 3: So S√°nh v√† Ch·ªçn Best Split

| Feature     | Threshold/Value | Gini Gain | Ranking |
| ----------- | --------------- | --------- | ------- |
| **Weekend** | 0 vs 1          | **0.32**  | ü•á #1   |
| BounceRates | 0.01            | 0.056     | #2      |
| PageValues  | 5.0             | 0.02      | #3      |

**K·∫øt lu·∫≠n:**

- ‚úÖ **Best Split:** `Weekend == 0` vs `Weekend == 1`
- ‚úÖ **Gini Gain cao nh·∫•t:** 0.32
- ‚úÖ **T·∫°o ra 2 pure nodes:** Perfect split!

---

## 5. Quy Tr√¨nh V·ªõi Dataset Th·ª±c T·∫ø (34 Features)

### 5.1. ∆Ø·ªõc T√≠nh S·ªë L∆∞·ª£ng T√≠nh To√°n

**Dataset th·ª±c t·∫ø:**

- 34 features (sau feature engineering)
- 12,330 m·∫´u
- Kho·∫£ng 20 numerical features, 14 categorical features

**V·ªõi m·ªói numerical feature:**

- S·ªë threshold candidates ‚âà s·ªë unique values / 2
- Trung b√¨nh: ~50-200 threshold m·ªói feature
- T·ªïng: 20 features √ó 100 threshold ‚âà **2,000 c√°ch split**

**V·ªõi m·ªói categorical feature:**

- Binary: 1 c√°ch split
- Multi-class (v√≠ d·ª• Month c√≥ 12 gi√° tr·ªã): 12 c√°ch split
- Trung b√¨nh: ~5 c√°ch split m·ªói feature
- T·ªïng: 14 features √ó 5 c√°ch ‚âà **70 c√°ch split**

**T·ªïng s·ªë t√≠nh to√°n:**

- 2,000 + 70 = **~2,070 l·∫ßn t√≠nh Gini Gain** cho 1 split!

#### 5.1.1. Gi·∫£i Th√≠ch Chi Ti·∫øt: "~20 numerical features √ó ~100 threshold ‚âà 2,000 c√°ch split"

##### 1. **~20 numerical features**

Theo file n√†y:
- **34 features** sau feature engineering
- **~14 categorical features** (nh∆∞ `Month`, `VisitorType`, `Weekend`, `is_q4`, `quarter`, ...)
- **~20 numerical features** = 34 - 14 (v√≠ d·ª•: `PageValues`, `BounceRates`, `total_duration`, `admin_duration_ratio`, ...)

##### 2. **~100 threshold cho m·ªói numerical feature**

C√°ch t√≠nh threshold:
- V·ªõi numerical feature, Decision Tree t·∫°o threshold candidates t·∫°i ƒëi·ªÉm gi·ªØa c√°c gi√° tr·ªã li√™n ti·∫øp
- V√≠ d·ª• v·ªõi `BounceRates`: `[0.0, 0.02, 0.05, 0.2, 0.2]` ‚Üí thresholds: `[0.01, 0.035, 0.125]` = **3 threshold candidates**

**T·∫°i sao ~100 threshold?**
- N·∫øu m·ªôt feature c√≥ kho·∫£ng 200 unique values ‚Üí c√≥ th·ªÉ t·∫°o ~200 threshold candidates
- N·∫øu c√≥ 50 unique values ‚Üí ~50 threshold candidates
- **Trung b√¨nh**: n·∫øu m·ªói numerical feature c√≥ kho·∫£ng 100 unique values ‚Üí **~100 threshold candidates**
- Scikit-learn c√≥ th·ªÉ gi·ªõi h·∫°n s·ªë threshold (v√≠ d·ª• t·ªëi ƒëa 256) ƒë·ªÉ t·ªëi ∆∞u t·ªëc ƒë·ªô

##### 3. **20 √ó 100 = 2,000 c√°ch split**

Logic:
- V·ªõi m·ªói numerical feature, th·ª≠ m·ªói threshold ‚Üí m·ªói threshold = **1 c√°ch split**
- 20 features √ó 100 threshold/feature = **2,000 c√°ch split c·∫ßn th·ª≠**

**V√≠ d·ª• minh h·ªça:**
```
Feature: PageValues
- Threshold 1: PageValues <= 5.0 ‚Üí Split #1
- Threshold 2: PageValues <= 10.0 ‚Üí Split #2
- Threshold 3: PageValues <= 15.0 ‚Üí Split #3
...
- Threshold 100: PageValues <= 500.0 ‚Üí Split #100
‚Üí T·ªïng: 100 c√°ch split cho PageValues

L√†m t∆∞∆°ng t·ª± cho 20 numerical features ‚Üí 20 √ó 100 = 2,000 c√°ch split
```

##### 4. **ƒê√¢y ch·ªâ l√† ∆∞·ªõc t√≠nh**

Trong th·ª±c t·∫ø:
- Feature c√≥ √≠t unique values ‚Üí √≠t threshold h∆°n
- Feature c√≥ nhi·ªÅu unique values ‚Üí nhi·ªÅu threshold h∆°n
- Scikit-learn c√≥ th·ªÉ gi·ªõi h·∫°n s·ªë threshold ƒë·ªÉ t·ªëi ∆∞u

**~100 threshold/feature** l√† ∆∞·ªõc t√≠nh trung b√¨nh d·ª±a tr√™n:
- Dataset c√≥ 12,330 m·∫´u
- C√°c numerical features c√≥ ph√¢n b·ªë ƒëa d·∫°ng
- C√≥ th·ªÉ √°p d·ª•ng gi·ªõi h·∫°n t·ªëi ƒëa (v√≠ d·ª• 256 thresholds)

##### T√≥m t·∫Øt

| Th√†nh ph·∫ßn | Gi·∫£i th√≠ch |
|------------|------------|
| **20 numerical features** | 34 t·ªïng features - 14 categorical ‚âà 20 numerical |
| **~100 threshold/feature** | ∆Ø·ªõc t√≠nh trung b√¨nh (d·ª±a tr√™n s·ªë unique values) |
| **2,000 c√°ch split** | 20 √ó 100 = 2,000 c√°ch split c·∫ßn th·ª≠ v√† t√≠nh Gini Gain |

Con s·ªë n√†y th·ªÉ hi·ªán ƒë·ªô ph·ª©c t·∫°p: ƒë·ªÉ ch·ªçn best split t·∫°i 1 node, Decision Tree c·∫ßn th·ª≠ v√† t√≠nh Gini Gain cho kho·∫£ng **2,000 c√°ch split kh√°c nhau**.

---

### 5.2. Pseudo-code ƒê·∫ßy ƒê·ªß

```python
def find_best_split(X, y):
    """
    T√¨m best split cho m·ªôt node

    Args:
        X: DataFrame v·ªõi 34 features
        y: Series labels (Revenue: 0 ho·∫∑c 1)

    Returns:
        (best_feature, best_threshold, best_gain)
    """
    # B∆∞·ªõc 1: T√≠nh Gini c·ªßa node cha
    parent_labels = y.tolist()
    gini_parent = gini_impurity(parent_labels)

    # B∆∞·ªõc 2: Kh·ªüi t·∫°o
    best_gain = -1
    best_feature = None
    best_threshold = None

    # B∆∞·ªõc 3: Duy·ªát qua t·∫•t c·∫£ features
    for feature_name in X.columns:  # 34 features
        feature_values = X[feature_name].values

        # Ki·ªÉm tra lo·∫°i feature
        if is_numerical(feature_values):
            # NUMERICAL FEATURE
            # S·∫Øp x·∫øp v√† t·∫°o thresholds
            sorted_values = np.sort(np.unique(feature_values))
            thresholds = [(sorted_values[i] + sorted_values[i+1]) / 2
                         for i in range(len(sorted_values)-1)]

            # Th·ª≠ t·ª´ng threshold
            for threshold in thresholds:
                # Split
                left_mask = feature_values <= threshold
                right_mask = ~left_mask

                left_labels = y[left_mask].tolist()
                right_labels = y[right_mask].tolist()

                # T√≠nh Gini Gain
                children = [left_labels, right_labels]
                weighted_gini = weighted_gini_impurity(children)
                gain = gini_parent - weighted_gini

                # C·∫≠p nh·∫≠t best
                if gain > best_gain:
                    best_gain = gain
                    best_feature = feature_name
                    best_threshold = threshold

        else:
            # CATEGORICAL FEATURE
            unique_values = np.unique(feature_values)

            # Th·ª≠ split theo t·ª´ng gi√° tr·ªã
            for value in unique_values:
                # Split
                left_mask = feature_values == value
                right_mask = ~left_mask

                left_labels = y[left_mask].tolist()
                right_labels = y[right_mask].tolist()

                # T√≠nh Gini Gain
                children = [left_labels, right_labels]
                weighted_gini = weighted_gini_impurity(children)
                gain = gini_parent - weighted_gini

                # C·∫≠p nh·∫≠t best
                if gain > best_gain:
                    best_gain = gain
                    best_feature = feature_name
                    best_threshold = value

    # B∆∞·ªõc 4: Tr·∫£ v·ªÅ best split
    return best_feature, best_threshold, best_gain
```

---

## 6. T·ªëi ∆Øu H√≥a (Optimization)

Trong th·ª±c t·∫ø, c√°c th∆∞ vi·ªán nh∆∞ scikit-learn s·ª≠ d·ª•ng nhi·ªÅu k·ªπ thu·∫≠t t·ªëi ∆∞u:

### 6.1. Ch·ªâ Th·ª≠ M·ªôt S·ªë Threshold Candidates

- Kh√¥ng th·ª≠ t·∫•t c·∫£ threshold, ch·ªâ th·ª≠ m·ªôt s·ªë gi√° tr·ªã ƒë·∫°i di·ªán
- V√≠ d·ª•: ch·ªâ th·ª≠ 256 threshold cho m·ªói numerical feature

### 6.2. Early Stopping

- N·∫øu t√¨m th·∫•y Gini Gain = Gini(parent) ‚Üí Perfect split, d·ª´ng ngay

### 6.3. Parallel Processing

- T√≠nh to√°n song song cho nhi·ªÅu features/thresholds

### 6.4. Sampling (trong Random Forest)

- Ch·ªâ x√©t m·ªôt t·∫≠p con features (v√≠ d·ª•: ‚àö34 ‚âà 6 features)
- Gi·∫£m ƒë√°ng k·ªÉ s·ªë l∆∞·ª£ng t√≠nh to√°n

---

## 7. T√≥m T·∫Øt

### Quy Tr√¨nh Ch√≠nh:

1. ‚úÖ **T√≠nh Gini(parent)** c·ªßa node c·∫ßn split
2. ‚úÖ **V·ªõi m·ªói feature (34 features):**
   - N·∫øu numerical: th·ª≠ nhi·ªÅu threshold ‚Üí t√≠nh Gini Gain
   - N·∫øu categorical: th·ª≠ c√°c c√°ch split ‚Üí t√≠nh Gini Gain
3. ‚úÖ **So s√°nh** t·∫•t c·∫£ Gini Gain
4. ‚úÖ **Ch·ªçn** feature + threshold c√≥ **Gini Gain cao nh·∫•t**
5. ‚úÖ **Split** node th√†nh children nodes

### ƒê·ªô Ph·ª©c T·∫°p:

- **S·ªë t√≠nh to√°n:** ~2,000-3,000 l·∫ßn t√≠nh Gini Gain cho 1 split
- **Time complexity:** O(n_features √ó n_samples √ó log(n_samples))
- **Space complexity:** O(n_samples)

### K·∫øt Qu·∫£:

- Decision Tree ch·ªçn ƒë∆∞·ª£c **best feature + threshold** t·∫°i m·ªói node
- Build tree t·ª´ root ƒë·∫øn leaves
- M·ªói split gi·∫£m Gini Impurity nhi·ªÅu nh·∫•t c√≥ th·ªÉ

---

## T√†i Li·ªáu Tham Kh·∫£o

- `docs/gini_explained.md`: Gi·∫£i th√≠ch chi ti·∫øt v·ªÅ Gini Impurity
- `docs/gain_calculation_3_groups.md`: C√°ch t√≠nh Gain cho nhi·ªÅu nh√≥m
- `docs/why_6_features.md`: T·∫°i sao v√≠ d·ª• ch·ªâ d√πng 6 features
